{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6db272-3cdc-4702-8c49-3cf544c687d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Модуль для системы вопросно-ответных задач (QA), включая:\n",
    "- Загрузку и предобработку данных\n",
    "- Дообучение SentenceBERT для оценки схожести текстов\n",
    "- Извлечение релевантного контекста\n",
    "- Выбор наилучшего ответа на вопрос\n",
    "- Оценку качества модели\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cdd9ef6-66e3-4138-b1e4-55d6b7e10b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    \"\"\"\n",
    "    Класс для загрузки и проверки данных из JSONL файлов.\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir (str): Путь к директории с данными\n",
    "        val_path (str): Полный путь к файлу валидационных данных\n",
    "        test_path (str): Полный путь к файлу тестовых данных\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir: str = \"data\"):\n",
    "        \"\"\"\n",
    "        Инициализирует загрузчик данных для валидационного и тестового наборов.\n",
    "        \n",
    "        Args:\n",
    "            data_dir (str): Путь к директории, содержащей val.jsonl и test.jsonl\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.val_path = os.path.join(data_dir, \"val.jsonl\")\n",
    "        self.test_path = os.path.join(data_dir, \"test.jsonl\")\n",
    "        \n",
    "        # Проверка существования файлов\n",
    "        if not os.path.exists(self.val_path):\n",
    "            raise FileNotFoundError(f\"Файл val.jsonl не найден в {data_dir}\")\n",
    "        if not os.path.exists(self.test_path):\n",
    "            raise FileNotFoundError(f\"Файл test.jsonl не найден в {data_dir}\")\n",
    "\n",
    "    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Загружает val.jsonl и test.jsonl как DataFrames.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: (valid_df, test_df)\n",
    "        \"\"\"\n",
    "        valid_df = pd.read_json(self.val_path, lines=True)\n",
    "        test_df = pd.read_json(self.test_path, lines=True)\n",
    "        \n",
    "        print(f\"Загружено:\")\n",
    "        print(f\"- Валидационные данные: {len(valid_df)} записей\")\n",
    "        print(f\"- Тестовые данные: {len(test_df)} записей\")\n",
    "        \n",
    "        return valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aad890a-46d0-47aa-9b7f-83314111033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Класс для предварительной обработки текста.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_numbered_sentences(text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Разделяет текст на предложения, которые начинаются с цифр в скобках (например, \"(1)\").\n",
    "        \n",
    "        Args:\n",
    "            text (str): Входной текст для разделения\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: Список предложений без номеров\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "        text = ' '.join(text.split())  # Удаление лишних пробелов\n",
    "        sentences = re.split(r'(?<!\\()\\s*\\(\\d+\\)\\s*', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d59f96-e06e-4d44-8f79-5e376c9fc75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Кастомный Dataset класс для хранения токенизированных данных и меток.\n",
    "    Наследуется от torch.utils.data.Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings: Dict[str, List[int]], labels: List[int]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encodings (Dict): Токенизированные тексты (input_ids, attention_mask и т.д.)\n",
    "            labels (List): Список меток для обучения\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Возвращает элемент датасета по индексу.\"\"\"\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Возвращает количество элементов в датасете.\"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2017db26-84f5-401e-a8d6-71d663f519ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBERT:\n",
    "    \"\"\"\n",
    "    Класс для работы с SentenceBERT моделью:\n",
    "    - Генерация эмбеддингов предложений\n",
    "    - Расчет схожести между предложениями\n",
    "    - Дообучение на специфичных данных\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_name: str = \"DeepPavlov/rubert-base-cased\",\n",
    "                 train_data: List[Tuple[str, str, int]] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name (str): Название предобученной модели\n",
    "            train_data (List): Данные для дообучения в формате [(текст1, текст2, метка), ...]\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        \n",
    "        if train_data:\n",
    "            self._train_model(model_name, train_data)\n",
    "    \n",
    "    def _train_model(self, model_name: str, train_data: List[Tuple[str, str, int]]):\n",
    "        \"\"\"Дообучение модели на предоставленных данных.\"\"\"\n",
    "        print(\"Дообучение SentenceBERT...\")\n",
    "        \n",
    "        # Подготовка данных\n",
    "        texts = [f\"{x[0]} [SEP] {x[1]}\" for x in train_data]\n",
    "        labels = [x[2] for x in train_data]\n",
    "        \n",
    "        # Токенизация\n",
    "        encodings = self.tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "        dataset = QADataset(encodings, labels)\n",
    "        \n",
    "        # Модель для классификации\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,  \n",
    "            num_labels=2\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Параметры обучения\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=60,\n",
    "            save_steps=10_000,\n",
    "            save_total_limit=2,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        self.model = model.base_model  # Используем базовую модель после обучения\n",
    "    \n",
    "    def embed(self, sentences: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Генерирует эмбеддинги для списка предложений.\n",
    "        \n",
    "        Args:\n",
    "            sentences (List[str]): Список предложений для эмбеддинга\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Тензор с эмбеддингами предложений\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            sentences, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        return self._mean_pooling(outputs, inputs['attention_mask'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def _mean_pooling(model_output: Any, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Применяет mean pooling к выходу модели для получения эмбеддингов предложений.\n",
    "        \n",
    "        Args:\n",
    "            model_output: Выход модели BERT\n",
    "            attention_mask: Маска внимания\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Усредненные эмбеддинги\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = (\n",
    "            attention_mask\n",
    "            .unsqueeze(-1)\n",
    "            .expand(token_embeddings.size())\n",
    "            .float()\n",
    "        )\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def similarity(self, sentence1: str, sentence2: str) -> float:\n",
    "        \"\"\"\n",
    "        Вычисляет косинусную схожесть между двумя предложениями.\n",
    "        \n",
    "        Args:\n",
    "            sentence1 (str): Первое предложение\n",
    "            sentence2 (str): Второе предложение\n",
    "            \n",
    "        Returns:\n",
    "            float: Косинусная схожесть между -1 и 1\n",
    "        \"\"\"\n",
    "        emb1, emb2 = self.embed([sentence1, sentence2])\n",
    "        return torch.cosine_similarity(emb1, emb2, dim=0).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "557e9998-69b3-4377-8c57-cec87125d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QASystem:\n",
    "    \"\"\"\n",
    "    Основной класс вопросно-ответной системы.\n",
    "    Объединяет все компоненты для обработки вопросов и выбора ответов.\n",
    "    \"\"\"\n",
    "    def __init__(self, need_tune: bool = True, data_path: str = \"data\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            need_tune (bool): Нужно ли дообучать SentenceBERT\n",
    "            data_path (str): Путь к директории с данными\n",
    "        \"\"\"\n",
    "        self.dataset_loader = DatasetLoader(data_path)\n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "        \n",
    "        # Подготовка данных и инициализация\n",
    "        train_data = self._prepare_training_data()\n",
    "        self.sbert = SentenceBERT(train_data=train_data if need_tune else None)\n",
    "        \n",
    "        # Инициализация QA модели\n",
    "        self.qa_model = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=\"AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru\",\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            max_answer_len=500,\n",
    "            handle_impossible_answer=False\n",
    "        )\n",
    "    \n",
    "    def _prepare_training_data(self) -> List[Tuple[str, str, int]]:\n",
    "        \"\"\"\n",
    "        Подготавливает данные для дообучения SentenceBERT.\n",
    "        \n",
    "        Returns:\n",
    "            List[Tuple[str, str, int]]: Список пар текстов с метками\n",
    "        \"\"\"\n",
    "        train_df, _ = self.dataset_loader.load_data()\n",
    "        training_pairs = []\n",
    "        \n",
    "        for _, row in train_df.iterrows():\n",
    "            passage = row['passage']\n",
    "            text = passage['text']\n",
    "            questions = passage['questions']\n",
    "            \n",
    "            for q in questions:\n",
    "                question = q['question']\n",
    "                answers = q['answers']\n",
    "                \n",
    "                pos_answers = [a['text'] for a in answers if a['label'] == 1]\n",
    "                neg_answers = [a['text'] for a in answers if a['label'] == 0]\n",
    "                \n",
    "                # Пары вопрос-ответ\n",
    "                for pos in pos_answers:\n",
    "                    training_pairs.append((question, pos, 1))\n",
    "                for neg in neg_answers:\n",
    "                    training_pairs.append((question, neg, 0))\n",
    "                \n",
    "                # Пары ответ-контекст\n",
    "                sentences = self.text_preprocessor.split_numbered_sentences(text)\n",
    "                if sentences:\n",
    "                    context = ' '.join(sentences[:3])\n",
    "                    for pos in pos_answers:\n",
    "                        training_pairs.append((pos, context, 1))\n",
    "                    for neg in neg_answers:\n",
    "                        training_pairs.append((neg, context, 0))\n",
    "        \n",
    "        return training_pairs\n",
    "    \n",
    "    def get_relevant_context(self, question: str, full_text: str, top_n: int = 7) -> str:\n",
    "        \"\"\"\n",
    "        Извлекает наиболее релевантные предложения из текста для заданного вопроса.\n",
    "        \n",
    "        Args:\n",
    "            question (str): Вопрос\n",
    "            full_text (str): Полный текст для поиска контекста\n",
    "            top_n (int): Количество возвращаемых предложений\n",
    "            \n",
    "        Returns:\n",
    "            str: Конкатенированные наиболее релевантные предложения\n",
    "        \"\"\"\n",
    "        sentences = self.text_preprocessor.split_numbered_sentences(full_text)\n",
    "        if not sentences:\n",
    "            return \"\"\n",
    "        \n",
    "        # Вычисляем схожесть каждого предложения с вопросом\n",
    "        similarities = []\n",
    "        for sent in sentences:\n",
    "            try:\n",
    "                sim = self.sbert.similarity(question, sent)\n",
    "                similarities.append(sim)\n",
    "            except:\n",
    "                similarities.append(0)\n",
    "        \n",
    "        # Выбираем топ-N наиболее релевантных предложений\n",
    "        top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "        return ' '.join([sentences[i] for i in top_indices])\n",
    "    \n",
    "    def select_best_answer(self, question: str, context: str, answer_options: List[Dict]) -> Tuple[Dict, float]:\n",
    "        \"\"\"\n",
    "        Выбирает наилучший ответ из предложенных вариантов.\n",
    "        \n",
    "        Args:\n",
    "            question (str): Вопрос\n",
    "            context (str): Контекст для поиска ответа\n",
    "            answer_options (List[Dict]): Список вариантов ответов\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[Dict, float]: Лучший ответ и его схожесть с предсказанием модели\n",
    "        \"\"\"\n",
    "        try:\n",
    "            qa_result = self.qa_model(question=question, context=context)\n",
    "            model_answer = qa_result[\"answer\"]\n",
    "        except:\n",
    "            model_answer = \"\"\n",
    "        \n",
    "        # Сравниваем с вариантами ответов\n",
    "        best_answer = max(\n",
    "            answer_options,\n",
    "            key=lambda x: self.sbert.similarity(model_answer, x[\"text\"])\n",
    "        )\n",
    "        similarity = self.sbert.similarity(model_answer, best_answer[\"text\"])\n",
    "        \n",
    "        return best_answer, similarity\n",
    "    \n",
    "    def process_passage(self, passage: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Обрабатывает один отрывок текста со всеми вопросами.\n",
    "        \n",
    "        Args:\n",
    "            passage (Dict): Отрывок текста с вопросами\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Результаты обработки всех вопросов\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        text = passage[\"text\"]\n",
    "        \n",
    "        for question_data in passage[\"questions\"]:\n",
    "            question = question_data[\"question\"]\n",
    "            answer_options = question_data[\"answers\"]\n",
    "            \n",
    "            context = self.get_relevant_context(question, text)\n",
    "            best_answer, similarity = self.select_best_answer(question, context, answer_options)\n",
    "            \n",
    "            correct_answers = [\n",
    "                {\"text\": ans[\"text\"], \"idx\": ans[\"idx\"]} \n",
    "                for ans in answer_options if ans[\"label\"] == 1\n",
    "            ]\n",
    "            \n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'context': context,\n",
    "                'selected_answer': best_answer[\"text\"],\n",
    "                'selected_answer_idx': best_answer[\"idx\"],\n",
    "                'is_correct': bool(best_answer[\"label\"]),\n",
    "                'confidence': similarity,\n",
    "                'correct_answers': correct_answers,\n",
    "                'correct_answers_text': [ans[\"text\"] for ans in correct_answers],\n",
    "                'correct_answers_idx': [ans[\"idx\"] for ans in correct_answers]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_metrics(self, y_true: List[int], y_pred: List[int]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Вычисляет метрики качества модели.\n",
    "        \n",
    "        Args:\n",
    "            y_true (List): Истинные метки\n",
    "            y_pred (List): Предсказанные метки\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Словарь с метриками (f1, accuracy)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'f1': f1_score(y_true, y_pred, average='macro'),\n",
    "            'accuracy': sum(np.array(y_true) == np.array(y_pred)) / len(y_true)\n",
    "        }\n",
    "    \n",
    "    def valid(self, dataset: List[Dict]) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Оценивает качество модели на валидационном наборе.\n",
    "        \n",
    "        Args:\n",
    "            dataset (List): Список отрывков с вопросами\n",
    "            \n",
    "        Returns:\n",
    "            Tuple: DataFrame с результатами и словарь метрик\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for row in tqdm(dataset, desc=\"Оценка\"):\n",
    "            passage_results = self.process_passage(row['passage'])\n",
    "            results.extend(passage_results)\n",
    "            \n",
    "            for res in passage_results:\n",
    "                y_true.append(res['is_correct'])\n",
    "                y_pred.append(1 if res['confidence'] > 0.5 else 0)\n",
    "        \n",
    "        metrics = self.calculate_metrics(y_true, y_pred)\n",
    "        print(f\"\\nМетрики качества:\")\n",
    "        print(f\"F1-мера: {metrics['f1']:.4f}\")\n",
    "        print(f\"Точность: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Проанализировано {len(results)} вопросов\")\n",
    "        \n",
    "        return pd.DataFrame(results), metrics\n",
    "    \n",
    "    def eval(self, dataset: List[Dict], output_file: str = \"predictions.jsonl\") -> str:\n",
    "        \"\"\"\n",
    "        Генерирует предсказания для тестового набора и сохраняет в файл.\n",
    "        \n",
    "        Args:\n",
    "            dataset (List): Список отрывков с вопросами\n",
    "            output_file (str): Путь для сохранения результатов\n",
    "            \n",
    "        Returns:\n",
    "            str: Сообщение о завершении\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for row in tqdm(dataset, desc=\"Генерация предсказаний\"):\n",
    "            passage = row['passage']\n",
    "            text = passage['text']\n",
    "            \n",
    "            for question_data in passage[\"questions\"]:\n",
    "                question = question_data[\"question\"]\n",
    "                answer_options = question_data[\"answers\"]\n",
    "                \n",
    "                context = self.get_relevant_context(question, text)\n",
    "                best_answer, similarity = self.select_best_answer(question, context, answer_options)\n",
    "                \n",
    "                results.append({\n",
    "                    'question_idx': question_data['idx'],\n",
    "                    'question': question,\n",
    "                    'context': context,\n",
    "                    'selected_answer': best_answer[\"text\"],\n",
    "                    'selected_answer_idx': best_answer[\"idx\"],\n",
    "                    'confidence': similarity,\n",
    "                    'all_answers': [\n",
    "                        {'text': ans['text'], 'idx': ans['idx']} \n",
    "                        for ans in answer_options\n",
    "                    ]\n",
    "                })\n",
    "        print(results)\n",
    "        \n",
    "        # Сохраняем в JSONL файл\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for res in results:\n",
    "                f.write(json.dumps(res, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        return f\"Предсказания сохранены в {output_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d573aa-7176-48f1-aebb-bce5750ccfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Основная функция для запуска QA системы.\"\"\"\n",
    "    qa_system = QASystem(need_tune=False)\n",
    "    valid_df, test_df = qa_system.dataset_loader.load_data()\n",
    "    \n",
    "    # Оценка на тестовых данных (с проверкой правильности)\n",
    "    # P.S. эксперименты проводились на этапе подбора моделей и гиперпараметров\n",
    "    valid_results, metrics = qa_system.valid(valid_df.to_dict('records'))\n",
    "    valid_results.to_csv(\"qa_results.csv\", index=False)\n",
    "    \n",
    "    # Сохранение предсказаний без проверки правильности\n",
    "    qa_system.eval(test_df.to_dict('records'), \"predictions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ee5826-87be-47e7-ad2e-b69ab73d3889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено:\n",
      "- Валидационные данные: 100 записей\n",
      "- Тестовые данные: 322 записей\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено:\n",
      "- Валидационные данные: 100 записей\n",
      "- Тестовые данные: 322 записей\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка: 100%|█████████████████████████████████████████████| 100/100 [09:49<00:00,  5.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики качества:\n",
      "F1-мера: 0.6524\n",
      "Точность: 0.7164\n",
      "Проанализировано 529 вопросов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Генерация предсказаний: 100%|█████████████████████████████| 322/322 [32:27<00:00,  6.05s/it]\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
